{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Ditection model\n",
    "\n",
    "* tensorflow model\n",
    "* scikit learn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data is collected from the spam.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding utf-8 didn't work\n",
      "Successfully read using encoding: ISO-8859-1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1                                                 v2 Unnamed: 2  \\\n",
       "0      ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1      ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3      ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "...    ...                                                ...        ...   \n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n",
       "5568   ham              Will Ì_ b going to esplanade fr home?        NaN   \n",
       "5569   ham  Pity, * was in mood for that. So...any other s...        NaN   \n",
       "5570   ham  The guy did some bitching but I acted like i'd...        NaN   \n",
       "5571   ham                         Rofl. Its true to its name        NaN   \n",
       "\n",
       "     Unnamed: 3 Unnamed: 4  \n",
       "0           NaN        NaN  \n",
       "1           NaN        NaN  \n",
       "2           NaN        NaN  \n",
       "3           NaN        NaN  \n",
       "4           NaN        NaN  \n",
       "...         ...        ...  \n",
       "5567        NaN        NaN  \n",
       "5568        NaN        NaN  \n",
       "5569        NaN        NaN  \n",
       "5570        NaN        NaN  \n",
       "5571        NaN        NaN  \n",
       "\n",
       "[5572 rows x 5 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Try different encodings until you find the correct one\n",
    "encodings_to_try = ['utf-8', 'ISO-8859-1', 'latin1', 'cp1252']\n",
    "\n",
    "for encoding in encodings_to_try:\n",
    "    try:\n",
    "        data = pd.read_csv(\"spam.csv\", encoding=encoding)\n",
    "        print(\"Successfully read using encoding:\", encoding)\n",
    "        break\n",
    "    except UnicodeDecodeError:\n",
    "        print(\"Encoding\", encoding, \"didn't work\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null columns are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1                                                 v2\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham              Will Ì_ b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nullcolumn = data.columns[data.isnull().any()]\n",
    "data = data.drop(columns=nullcolumn)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spam and ham is changed to the 1 and 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>1</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      v1                                                 v2\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro...\n",
       "...   ..                                                ...\n",
       "5567   1  This is the 2nd time we have tried 2 contact u...\n",
       "5568   0              Will Ì_ b going to esplanade fr home?\n",
       "5569   0  Pity, * was in mood for that. So...any other s...\n",
       "5570   0  The guy did some bitching but I acted like i'd...\n",
       "5571   0                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_dict = {'spam' : 1, 'ham' : 0}\n",
    "data['v1'] = data['v1'].replace(replace_dict)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using train test split the data is plited for the testing and the training purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train , x_test , y_train , y_test = train_test_split(data[\"v1\"],data[\"v2\"],test_size = .30 , random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3900,)\n",
      "(3900,)\n",
      "(1672,)\n",
      "(1672,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(708     1\n",
       " 4338    0\n",
       " 5029    0\n",
       " 4921    0\n",
       " 2592    0\n",
       " 2275    0\n",
       " 1424    0\n",
       " 1216    1\n",
       " 5211    0\n",
       " 4743    0\n",
       " Name: v1, dtype: int64,\n",
       " 708     To review and KEEP the fantastic Nokia N-Gage ...\n",
       " 4338                   Just got outta class gonna go gym.\n",
       " 5029    Is there coming friday is leave for pongal?do ...\n",
       " 4921    Hi Dear Call me its urgnt. I don't know whats ...\n",
       " 2592    My friend just got here and says he's upping h...\n",
       " 2275           Is that on the telly? No its Brdget Jones!\n",
       " 1424                    Yes.. now only saw your message..\n",
       " 1216    You have 1 new voicemail. Please call 08719181...\n",
       " 5211                      It is only yesterday true true.\n",
       " 4743                               \\Thinking of u ;) x\\\"\"\n",
       " Name: v2, dtype: object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:10], y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text vectorization using the tensorflow keras layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization # after TensorFlow 2.6\n",
    "\n",
    "# Before TensorFlow 2.6\n",
    "# from tensorflow.keras.layers.experimental.preprocessing import TextVectorization \n",
    "# Note: in TensorFlow 2.6+, you no longer need \"layers.experimental.preprocessing\"\n",
    "# you can use: \"tf.keras.layers.TextVectorization\", see https://github.com/tensorflow/tensorflow/releases/tag/v2.6.0 for more\n",
    "\n",
    "# Use the default TextVectorization variables\n",
    "text_vectorizer = TextVectorization(max_tokens=None, # how many words in the vocabulary (all of the different words in your text)\n",
    "                                    standardize=\"lower_and_strip_punctuation\", # how to process text\n",
    "                                    split=\"whitespace\", # how to split tokens\n",
    "                                    ngrams=None, # create groups of n-words?\n",
    "                                    output_mode=\"int\", # how to map tokens to numbers\n",
    "                                    output_sequence_length=None) # how long should the output sequence of tokens be?\n",
    "                                    # pad_to_max_tokens=True) # Not valid if using max_tokens=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#average no of tokens\n",
    "round(sum([len(i.split()) for i in y_train])/len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_length = 10000\n",
    "max_length = 15\n",
    "\n",
    "# Create the TextVectorization layer\n",
    "text_vectorizer = TextVectorization(\n",
    "    max_tokens=max_vocab_length,\n",
    "    output_mode=\"int\",  # or \"binary\", \"tf-idf\", etc., depending on your needs\n",
    "    output_sequence_length=max_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text_vectorizer adapted to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the text vectorizer to the training text\n",
    "text_vectorizer.adapt(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['',\n",
       "  '[UNK]',\n",
       "  'to',\n",
       "  'i',\n",
       "  'you',\n",
       "  'a',\n",
       "  'the',\n",
       "  'u',\n",
       "  'and',\n",
       "  'in',\n",
       "  'is',\n",
       "  'me',\n",
       "  'my',\n",
       "  'for',\n",
       "  'your',\n",
       "  'of',\n",
       "  'it',\n",
       "  'have',\n",
       "  'call',\n",
       "  'on',\n",
       "  'are',\n",
       "  'that',\n",
       "  'now',\n",
       "  '2',\n",
       "  'im',\n",
       "  'so',\n",
       "  'not',\n",
       "  'but',\n",
       "  'at',\n",
       "  'or',\n",
       "  'ur',\n",
       "  'get',\n",
       "  'will',\n",
       "  'just',\n",
       "  'do',\n",
       "  'can',\n",
       "  'be',\n",
       "  'if',\n",
       "  'with',\n",
       "  'we',\n",
       "  'this',\n",
       "  'no',\n",
       "  'its',\n",
       "  'up',\n",
       "  'free',\n",
       "  'go',\n",
       "  'when',\n",
       "  'ltgt',\n",
       "  '4',\n",
       "  'ok',\n",
       "  'from',\n",
       "  'what',\n",
       "  'dont',\n",
       "  'all',\n",
       "  'how',\n",
       "  'know',\n",
       "  'out',\n",
       "  'then',\n",
       "  'like',\n",
       "  'am',\n",
       "  'got',\n",
       "  'ill',\n",
       "  'come',\n",
       "  'was',\n",
       "  'only',\n",
       "  'good',\n",
       "  'time',\n",
       "  'love',\n",
       "  'text',\n",
       "  'send',\n",
       "  'there',\n",
       "  'day',\n",
       "  'want',\n",
       "  'going',\n",
       "  'txt',\n",
       "  'home',\n",
       "  'by',\n",
       "  'still',\n",
       "  'he',\n",
       "  'need',\n",
       "  'lor',\n",
       "  'as',\n",
       "  'one',\n",
       "  'sorry',\n",
       "  'see',\n",
       "  'about',\n",
       "  'r',\n",
       "  'stop',\n",
       "  'reply',\n",
       "  'mobile',\n",
       "  'da',\n",
       "  'back',\n",
       "  'our',\n",
       "  'hi',\n",
       "  'n',\n",
       "  'today',\n",
       "  'well',\n",
       "  'please',\n",
       "  'new',\n",
       "  'think',\n",
       "  'Ì',\n",
       "  'cant',\n",
       "  'an',\n",
       "  'she',\n",
       "  'tell',\n",
       "  'phone',\n",
       "  'later',\n",
       "  'any',\n",
       "  'her',\n",
       "  'been',\n",
       "  'hey',\n",
       "  'they',\n",
       "  'some',\n",
       "  'here',\n",
       "  'claim',\n",
       "  'did',\n",
       "  'take',\n",
       "  'night',\n",
       "  'him',\n",
       "  'more',\n",
       "  'week',\n",
       "  'has',\n",
       "  'happy',\n",
       "  'dear',\n",
       "  'way',\n",
       "  'pls',\n",
       "  'thats',\n",
       "  'too',\n",
       "  'oh',\n",
       "  'much',\n",
       "  'tomorrow',\n",
       "  'had',\n",
       "  'd',\n",
       "  'who',\n",
       "  'where',\n",
       "  'make',\n",
       "  'work',\n",
       "  'number',\n",
       "  'yes',\n",
       "  'say',\n",
       "  'prize',\n",
       "  'msg',\n",
       "  'hope',\n",
       "  'give',\n",
       "  'great',\n",
       "  'already',\n",
       "  'should',\n",
       "  'meet',\n",
       "  'said',\n",
       "  'message',\n",
       "  'ive',\n",
       "  'ask',\n",
       "  'very',\n",
       "  'right',\n",
       "  'were',\n",
       "  'wat',\n",
       "  'c',\n",
       "  'after',\n",
       "  'really',\n",
       "  'last',\n",
       "  'babe',\n",
       "  'amp',\n",
       "  'why',\n",
       "  'them',\n",
       "  'won',\n",
       "  'lol',\n",
       "  'doing',\n",
       "  '1',\n",
       "  'didnt',\n",
       "  'cos',\n",
       "  'cash',\n",
       "  'morning',\n",
       "  'e',\n",
       "  'yeah',\n",
       "  'sure',\n",
       "  'pick',\n",
       "  'anything',\n",
       "  'also',\n",
       "  'sent',\n",
       "  'feel',\n",
       "  'win',\n",
       "  'thanks',\n",
       "  'over',\n",
       "  'nokia',\n",
       "  'miss',\n",
       "  'first',\n",
       "  'find',\n",
       "  'every',\n",
       "  'buy',\n",
       "  'around',\n",
       "  'again',\n",
       "  '3',\n",
       "  'would',\n",
       "  'which',\n",
       "  'life',\n",
       "  'before',\n",
       "  'b',\n",
       "  'went',\n",
       "  'something',\n",
       "  'off',\n",
       "  'next',\n",
       "  'let',\n",
       "  'keep',\n",
       "  'gonna',\n",
       "  'could',\n",
       "  'urgent',\n",
       "  'tonight',\n",
       "  'ya',\n",
       "  'waiting',\n",
       "  'wait',\n",
       "  'soon',\n",
       "  'per',\n",
       "  'nice',\n",
       "  'many',\n",
       "  'down',\n",
       "  'contact',\n",
       "  'chat',\n",
       "  'k',\n",
       "  'wan',\n",
       "  'place',\n",
       "  'money',\n",
       "  'ÌÏ',\n",
       "  'us',\n",
       "  'thing',\n",
       "  'his',\n",
       "  'x',\n",
       "  'leave',\n",
       "  'friends',\n",
       "  'youre',\n",
       "  'wish',\n",
       "  'told',\n",
       "  'getting',\n",
       "  'dun',\n",
       "  'sleep',\n",
       "  'service',\n",
       "  'even',\n",
       "  'done',\n",
       "  'class',\n",
       "  'care',\n",
       "  'wont',\n",
       "  'someone',\n",
       "  'late',\n",
       "  'help',\n",
       "  'hello',\n",
       "  'haha',\n",
       "  'guaranteed',\n",
       "  'customer',\n",
       "  'talk',\n",
       "  'same',\n",
       "  'name',\n",
       "  'havent',\n",
       "  'coming',\n",
       "  'year',\n",
       "  'tone',\n",
       "  'thought',\n",
       "  'smile',\n",
       "  'bit',\n",
       "  '16',\n",
       "  'yet',\n",
       "  'trying',\n",
       "  'things',\n",
       "  'people',\n",
       "  'line',\n",
       "  'holiday',\n",
       "  'heart',\n",
       "  'best',\n",
       "  'use',\n",
       "  'thk',\n",
       "  'sms',\n",
       "  'other',\n",
       "  'meeting',\n",
       "  'latest',\n",
       "  'into',\n",
       "  'having',\n",
       "  'finish',\n",
       "  'fine',\n",
       "  'few',\n",
       "  'v',\n",
       "  'min',\n",
       "  'may',\n",
       "  'man',\n",
       "  'lunch',\n",
       "  'live',\n",
       "  'draw',\n",
       "  'yup',\n",
       "  'try',\n",
       "  'special',\n",
       "  'person',\n",
       "  'hows',\n",
       "  'dat',\n",
       "  'y',\n",
       "  'receive',\n",
       "  'ready',\n",
       "  'mind',\n",
       "  'lar',\n",
       "  'gud',\n",
       "  'eat',\n",
       "  'cool',\n",
       "  'always',\n",
       "  'yo',\n",
       "  'never',\n",
       "  'mins',\n",
       "  '18',\n",
       "  'å£1000',\n",
       "  'shit',\n",
       "  'lot',\n",
       "  'liao',\n",
       "  'guess',\n",
       "  'check',\n",
       "  'car',\n",
       "  'camera',\n",
       "  '5',\n",
       "  '1st',\n",
       "  'wanna',\n",
       "  'two',\n",
       "  'stuff',\n",
       "  'speak',\n",
       "  'sir',\n",
       "  'room',\n",
       "  'reach',\n",
       "  'quite',\n",
       "  'play',\n",
       "  'luv',\n",
       "  'days',\n",
       "  'birthday',\n",
       "  'being',\n",
       "  'awarded',\n",
       "  '6',\n",
       "  'problem',\n",
       "  'probably',\n",
       "  'princess',\n",
       "  'orange',\n",
       "  'nothing',\n",
       "  'house',\n",
       "  'hes',\n",
       "  'half',\n",
       "  'forgot',\n",
       "  'face',\n",
       "  'early',\n",
       "  'box',\n",
       "  'big',\n",
       "  'because',\n",
       "  'actually',\n",
       "  'å£2000',\n",
       "  'world',\n",
       "  'word',\n",
       "  'without',\n",
       "  'watch',\n",
       "  'remember',\n",
       "  'real',\n",
       "  'po',\n",
       "  'once',\n",
       "  'look',\n",
       "  'guys',\n",
       "  'friend',\n",
       "  'ever',\n",
       "  'enjoy',\n",
       "  'dinner',\n",
       "  'cost',\n",
       "  'chance',\n",
       "  'better',\n",
       "  'baby',\n",
       "  '150ppm',\n",
       "  'weekend',\n",
       "  'than',\n",
       "  'start',\n",
       "  'shows',\n",
       "  'sat',\n",
       "  'pay',\n",
       "  'part',\n",
       "  'might',\n",
       "  'landline',\n",
       "  'juz',\n",
       "  'job',\n",
       "  'fuck',\n",
       "  'entry',\n",
       "  'bus',\n",
       "  'apply',\n",
       "  'another',\n",
       "  'aight',\n",
       "  'account',\n",
       "  '2nd',\n",
       "  'wot',\n",
       "  'whats',\n",
       "  'tones',\n",
       "  'todays',\n",
       "  'those',\n",
       "  'thanx',\n",
       "  'sweet',\n",
       "  'school',\n",
       "  'pa',\n",
       "  'missing',\n",
       "  'means',\n",
       "  'maybe',\n",
       "  'long',\n",
       "  'little',\n",
       "  'leh',\n",
       "  'left',\n",
       "  'kiss',\n",
       "  'hour',\n",
       "  'girl',\n",
       "  'end',\n",
       "  'does',\n",
       "  'award',\n",
       "  'ah',\n",
       "  '7',\n",
       "  'watching',\n",
       "  'tmr',\n",
       "  'thank',\n",
       "  'tcs',\n",
       "  'shall',\n",
       "  'selected',\n",
       "  'ringtone',\n",
       "  'put',\n",
       "  'offer',\n",
       "  'month',\n",
       "  'minutes',\n",
       "  'made',\n",
       "  'id',\n",
       "  'fun',\n",
       "  'everything',\n",
       "  'easy',\n",
       "  'dis',\n",
       "  'dad',\n",
       "  'came',\n",
       "  'called',\n",
       "  'able',\n",
       "  'å£150',\n",
       "  'xxx',\n",
       "  'xmas',\n",
       "  'wanted',\n",
       "  'till',\n",
       "  'texts',\n",
       "  'since',\n",
       "  'shopping',\n",
       "  'hear',\n",
       "  'hav',\n",
       "  'goes',\n",
       "  'bad',\n",
       "  'attempt',\n",
       "  'anyway',\n",
       "  'yourself',\n",
       "  'working',\n",
       "  'wif',\n",
       "  'weekly',\n",
       "  'town',\n",
       "  'top',\n",
       "  'stay',\n",
       "  'show',\n",
       "  'shes',\n",
       "  'sexy',\n",
       "  'says',\n",
       "  'plan',\n",
       "  'okay',\n",
       "  'most',\n",
       "  'jus',\n",
       "  'food',\n",
       "  'enough',\n",
       "  'dunno',\n",
       "  'double',\n",
       "  'collection',\n",
       "  'collect',\n",
       "  'believe',\n",
       "  'yesterday',\n",
       "  'wk',\n",
       "  'weeks',\n",
       "  'wake',\n",
       "  'update',\n",
       "  'tv',\n",
       "  'together',\n",
       "  'times',\n",
       "  'office',\n",
       "  'must',\n",
       "  'join',\n",
       "  'huh',\n",
       "  'god',\n",
       "  'doesnt',\n",
       "  'den',\n",
       "  'de',\n",
       "  'abt',\n",
       "  '9',\n",
       "  '8007',\n",
       "  '500',\n",
       "  'words',\n",
       "  'trip',\n",
       "  'test',\n",
       "  'simple',\n",
       "  's',\n",
       "  'rate',\n",
       "  'price',\n",
       "  'plz',\n",
       "  'plus',\n",
       "  'pain',\n",
       "  'network',\n",
       "  'ltdecimalgt',\n",
       "  'important',\n",
       "  'hot',\n",
       "  'goin',\n",
       "  'details',\n",
       "  'code',\n",
       "  'change',\n",
       "  'calls',\n",
       "  'away',\n",
       "  'afternoon',\n",
       "  'years',\n",
       "  'worry',\n",
       "  'video',\n",
       "  'tot',\n",
       "  'took',\n",
       "  'though',\n",
       "  'these',\n",
       "  'theres',\n",
       "  'tc',\n",
       "  'sch',\n",
       "  'saying',\n",
       "  'saw',\n",
       "  'ring',\n",
       "  'online',\n",
       "  'okie',\n",
       "  'nite',\n",
       "  'movie',\n",
       "  'm',\n",
       "  'lose',\n",
       "  'lei',\n",
       "  'haf',\n",
       "  'gift',\n",
       "  'g',\n",
       "  'else',\n",
       "  'either',\n",
       "  'drop',\n",
       "  'driving',\n",
       "  'drive',\n",
       "  'bt',\n",
       "  'brother',\n",
       "  'boy',\n",
       "  'book',\n",
       "  'between',\n",
       "  'bed',\n",
       "  'asked',\n",
       "  'alright',\n",
       "  '150p',\n",
       "  'å£100',\n",
       "  'wil',\n",
       "  'wen',\n",
       "  'tried',\n",
       "  'taking',\n",
       "  'shop',\n",
       "  'poly',\n",
       "  'order',\n",
       "  'old',\n",
       "  'noe',\n",
       "  'music',\n",
       "  'mob',\n",
       "  'making',\n",
       "  'lots',\n",
       "  'lets',\n",
       "  'leaving',\n",
       "  'knw',\n",
       "  'guy',\n",
       "  'gr8',\n",
       "  'full',\n",
       "  'friendship',\n",
       "  'eve',\n",
       "  'drink',\n",
       "  'charge',\n",
       "  'bring',\n",
       "  'both',\n",
       "  'å£5000',\n",
       "  'yours',\n",
       "  'wants',\n",
       "  'walk',\n",
       "  'voucher',\n",
       "  'valid',\n",
       "  'until',\n",
       "  'til',\n",
       "  'sleeping',\n",
       "  'sister',\n",
       "  'sae',\n",
       "  'missed',\n",
       "  'makes',\n",
       "  'lucky',\n",
       "  'looking',\n",
       "  'lesson',\n",
       "  'land',\n",
       "  'hours',\n",
       "  'hair',\n",
       "  'game',\n",
       "  'forget',\n",
       "  'finished',\n",
       "  'final',\n",
       "  'evening',\n",
       "  'email',\n",
       "  'each',\n",
       "  'delivery',\n",
       "  'colour',\n",
       "  'busy',\n",
       "  'await',\n",
       "  'aft',\n",
       "  '86688',\n",
       "  'å£500',\n",
       "  'å£250',\n",
       "  'wonderful',\n",
       "  'wife',\n",
       "  'while',\n",
       "  'whatever',\n",
       "  'vouchers',\n",
       "  'visit',\n",
       "  'tscs',\n",
       "  'treat',\n",
       "  'thinking',\n",
       "  'started',\n",
       "  'sounds',\n",
       "  'sis',\n",
       "  'set',\n",
       "  'services',\n",
       "  're',\n",
       "  'question',\n",
       "  'pub',\n",
       "  'project',\n",
       "  'pretty',\n",
       "  'pounds',\n",
       "  'post',\n",
       "  'pic',\n",
       "  'operator',\n",
       "  'nope',\n",
       "  'neva',\n",
       "  'mum',\n",
       "  'mobileupd8',\n",
       "  'mean',\n",
       "  'mail',\n",
       "  'gd',\n",
       "  'dude',\n",
       "  'date',\n",
       "  'coz',\n",
       "  'comes',\n",
       "  'bout',\n",
       "  'bonus',\n",
       "  'area',\n",
       "  'ard',\n",
       "  'address',\n",
       "  'wkly',\n",
       "  'winner',\n",
       "  'used',\n",
       "  'unsubscribe',\n",
       "  'unlimited',\n",
       "  'true',\n",
       "  'takes',\n",
       "  'sun',\n",
       "  'story',\n",
       "  'snow',\n",
       "  'smth',\n",
       "  'smiling',\n",
       "  'run',\n",
       "  'points',\n",
       "  'player',\n",
       "  'pics',\n",
       "  'phones',\n",
       "  'oso',\n",
       "  'oredi',\n",
       "  'open',\n",
       "  'minute',\n",
       "  'mine',\n",
       "  'messages',\n",
       "  'log',\n",
       "  'isnt',\n",
       "  'hurt',\n",
       "  'hungry',\n",
       "  'head',\n",
       "  'happened',\n",
       "  'happen',\n",
       "  'gone',\n",
       "  'frnd',\n",
       "  'friday',\n",
       "  'fri',\n",
       "  'feeling',\n",
       "  'fast',\n",
       "  'far',\n",
       "  'everyone',\n",
       "  'congratulations',\n",
       "  'company',\n",
       "  'college',\n",
       "  'choose',\n",
       "  'case',\n",
       "  'break',\n",
       "  'boytoy',\n",
       "  'beautiful',\n",
       "  '12hrs',\n",
       "  '08000839402',\n",
       "  'å£350',\n",
       "  'wats',\n",
       "  'usf',\n",
       "  'uncle',\n",
       "  'tomo',\n",
       "  'tho',\n",
       "  'ten',\n",
       "  'telling',\n",
       "  'st',\n",
       "  'song',\n",
       "  'second',\n",
       "  'sea',\n",
       "  'sad',\n",
       "  'pobox',\n",
       "  'pm',\n",
       "  'news',\n",
       "  'msgs',\n",
       "  'meant',\n",
       "  'loving',\n",
       "  'kind',\n",
       "  'ipod',\n",
       "  'invited',\n",
       "  'hand',\n",
       "  'ha',\n",
       "  'gas',\n",
       "  'fucking',\n",
       "  'frnds',\n",
       "  'fr',\n",
       "  'found',\n",
       "  'felt',\n",
       "  'fancy',\n",
       "  'enter',\n",
       "  'decided',\n",
       "  'dating',\n",
       "  'couple',\n",
       "  'content',\n",
       "  'congrats',\n",
       "  'club',\n",
       "  'close',\n",
       "  'card',\n",
       "  'calling',\n",
       "  'bday',\n",
       "  'awesome',\n",
       "  'available',\n",
       "  'answer',\n",
       "  '8',\n",
       "  '750',\n",
       "  '12',\n",
       "  '10p',\n",
       "  '100',\n",
       "  'yr',\n",
       "  'youll',\n",
       "  'yar',\n",
       "  'worth',\n",
       "  'woke',\n",
       "  'ugh',\n",
       "  'touch',\n",
       "  'tired',\n",
       "  'through',\n",
       "  't',\n",
       "  'search',\n",
       "  'saturday',\n",
       "  'rply',\n",
       "  'rock',\n",
       "  'rental',\n",
       "  'reason',\n",
       "  'read',\n",
       "  'plans',\n",
       "  'park',\n",
       "  'paper',\n",
       "  'opt',\n",
       "  'offers',\n",
       "  'nt',\n",
       "  'muz',\n",
       "  'mu',\n",
       "  'motorola',\n",
       "  'months',\n",
       "  'ltd',\n",
       "  'lovely',\n",
       "  'laptop',\n",
       "  'information',\n",
       "  'hold',\n",
       "  'hell',\n",
       "  'hard',\n",
       "  'gym',\n",
       "  'goodmorning',\n",
       "  'frm',\n",
       "  'freemsg',\n",
       "  'finally',\n",
       "  'family',\n",
       "  'extra',\n",
       "  'etc',\n",
       "  'eg',\n",
       "  'earlier',\n",
       "  'darlin',\n",
       "  'credit',\n",
       "  'crazy',\n",
       "  'crave',\n",
       "  'course',\n",
       "  'complimentary',\n",
       "  'comp',\n",
       "  'christmas',\n",
       "  'cause',\n",
       "  'caller',\n",
       "  'blue',\n",
       "  'bank',\n",
       "  'auction',\n",
       "  'anyone',\n",
       "  'alone',\n",
       "  'almost',\n",
       "  '150pmsg',\n",
       "  'yep',\n",
       "  'wrong',\n",
       "  'within',\n",
       "  'wid',\n",
       "  'wana',\n",
       "  'w',\n",
       "  'via',\n",
       "  'txts',\n",
       "  'th',\n",
       "  'surprise',\n",
       "  'statement',\n",
       "  'somebody',\n",
       "  'small',\n",
       "  'sitting',\n",
       "  'side',\n",
       "  'sending',\n",
       "  'seeing',\n",
       "  'save',\n",
       "  'savamob',\n",
       "  'rite',\n",
       "  'remove',\n",
       "  'reading',\n",
       "  'quiz',\n",
       "  'prob',\n",
       "  'private',\n",
       "  'picking',\n",
       "  'party',\n",
       "  'others',\n",
       "  'o',\n",
       "  'ntt',\n",
       "  'national',\n",
       "  'mobiles',\n",
       "  'mayb',\n",
       "  'mates',\n",
       "  'mate',\n",
       "  'match',\n",
       "  'mah',\n",
       "  'loads',\n",
       "  'least',\n",
       "  'john',\n",
       "  'jay',\n",
       "  'india',\n",
       "  'immediately',\n",
       "  'glad',\n",
       "  'father',\n",
       "  'f',\n",
       "  'expires',\n",
       "  'empty',\n",
       "  'direct',\n",
       "  'difficult',\n",
       "  'currently',\n",
       "  'cs',\n",
       "  'chikku',\n",
       "  'chennai',\n",
       "  'cheap',\n",
       "  'charity',\n",
       "  'carlos',\n",
       "  'bored',\n",
       "  'booked',\n",
       "  'bill',\n",
       "  'bathe',\n",
       "  'bath',\n",
       "  'ass',\n",
       "  'anytime',\n",
       "  'angry',\n",
       "  'age',\n",
       "  '87066',\n",
       "  '08000930705',\n",
       "  '0800',\n",
       "  'write',\n",
       "  'worries',\n",
       "  'whenever',\n",
       "  'voice',\n",
       "  'valued',\n",
       "  'unredeemed',\n",
       "  'uk',\n",
       "  'type',\n",
       "  'truth',\n",
       "  'tickets',\n",
       "  'thinkin',\n",
       "  'tel',\n",
       "  'supposed',\n",
       "  'summer',\n",
       "  'spend',\n",
       "  'sound',\n",
       "  'slowly',\n",
       "  'sick',\n",
       "  'shower',\n",
       "  'semester',\n",
       "  'sell',\n",
       "  'seen',\n",
       "  'sale',\n",
       "  'reached',\n",
       "  'rates',\n",
       "  'poor',\n",
       "  'police',\n",
       "  'point',\n",
       "  'outside',\n",
       "  'optout',\n",
       "  'numbers',\n",
       "  'nigeria',\n",
       "  'nah',\n",
       "  'myself',\n",
       "  'mrng',\n",
       "  'mr',\n",
       "  'mon',\n",
       "  'lost',\n",
       "  'listen',\n",
       "  'less',\n",
       "  'ldn',\n",
       "  'info',\n",
       "  'identifier',\n",
       "  'hmm',\n",
       "  'hiya',\n",
       "  'hit',\n",
       "  'hgsuite3422lands',\n",
       "  'happiness',\n",
       "  'grins',\n",
       "  'gotta',\n",
       "  'goto',\n",
       "  'gets',\n",
       "  'gay',\n",
       "  'games',\n",
       "  'fone',\n",
       "  'figure',\n",
       "  'fantastic',\n",
       "  'entered',\n",
       "  'ends',\n",
       "  'eh',\n",
       "  'drugs',\n",
       "  'dreams',\n",
       "  'discount',\n",
       "  'different',\n",
       "  'die',\n",
       "  'cum',\n",
       "  'comin',\n",
       "  'cd',\n",
       "  'camcorder',\n",
       "  'bslvyl',\n",
       "  'asking',\n",
       "  'askd',\n",
       "  'ans',\n",
       "  '25p',\n",
       "  '11mths',\n",
       "  '10pmin',\n",
       "  'å£800',\n",
       "  'å£200',\n",
       "  '\\x89Û',\n",
       "  'youve',\n",
       "  'yoga',\n",
       "  'yahoo',\n",
       "  'xx',\n",
       "  'wun',\n",
       "  'worried',\n",
       "  'workin',\n",
       "  'wonder',\n",
       "  'wiv',\n",
       "  'wit',\n",
       "  'welcome',\n",
       "  'weekends',\n",
       "  'walking',\n",
       "  'valentines',\n",
       "  'uve',\n",
       "  'using',\n",
       "  'user',\n",
       "  'under',\n",
       "  'train',\n",
       "  'tonite',\n",
       "  'terms',\n",
       "  'talking',\n",
       "  'support',\n",
       "  'sunshine',\n",
       "  'sunday',\n",
       "  'style',\n",
       "  'study',\n",
       "  'street',\n",
       "  'specially',\n",
       "  'sort',\n",
       "  'slave',\n",
       "  'sight',\n",
       "  'seems',\n",
       "  'secret',\n",
       "  'safe',\n",
       "  'quick',\n",
       "  'polys',\n",
       "  'pete',\n",
       "  'pc',\n",
       "  'pass',\n",
       "  'parents',\n",
       "  'orchard',\n",
       "  'opinion',\n",
       "  'omw',\n",
       "  'nyt',\n",
       "  'noon',\n",
       "  'ni8',\n",
       "  'net',\n",
       "  'needs',\n",
       "  'mrt',\n",
       "  'movies',\n",
       "  'move',\n",
       "  'monday',\n",
       "  ...],\n",
       " ['', '[UNK]', 'to', 'i', 'you'],\n",
       " ['02', '0125698789', '0121', '0089my', '0'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_in_vocab = text_vectorizer.get_vocabulary()\n",
    "top_5_words = word_in_vocab[:5]\n",
    "bottom_5_words = word_in_vocab[-5:]\n",
    "word_in_vocab , top_5_words , bottom_5_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer from the tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.core.embedding.Embedding at 0x1c3fe647b90>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(\n",
    "    input_dim = max_vocab_length,\n",
    "    output_dim = 128,\n",
    "    embeddings_initializer =\"uniform\",\n",
    "    input_length = max_length,\n",
    "    name = \"embedding\")\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random_sentence = random.choice(y_train)\n",
    "\n",
    "print(f\"radom sentence: {random_sentence}\")\n",
    "\n",
    "sample_embedded = embedding(text_vectorizer([random_sentence]))\n",
    "sample_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "708     To review and KEEP the fantastic Nokia N-Gage ...\n",
       "4338                   Just got outta class gonna go gym.\n",
       "Name: v2, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn model using the TfidfVectorizer and MultinomialNB in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create tokenization and modelling pipeline\n",
    "model_0 = Pipeline([\n",
    "                    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf\n",
    "                    (\"clf\", MultinomialNB()) # model the text\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_0.fit(y_train, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our baseline model achieves an accuracy of: 95.99%\n"
     ]
    }
   ],
   "source": [
    "baseline_score = model_0.score(y_test, x_test)\n",
    "print(f\"Our baseline model achieves an accuracy of: {baseline_score*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_preds = model_0.predict(y_test)\n",
    "baseline_preds[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of precision and recall and accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_recall_fscore_support\n",
    "\n",
    "def calculate_results(y_true , y_pred):\n",
    "    model_accuracy = accuracy_score(y_true , y_pred) * 100\n",
    "\n",
    "    model_precision , model_recall , model_f1 , _ = precision_recall_fscore_support(y_true , y_pred,average = \"weighted\")\n",
    "    model_results = {\n",
    "        \"accuracy\" : model_accuracy,\n",
    "        \"precision\" : model_precision,\n",
    "        \"recall\" : model_recall,\n",
    "        \"f1\" : model_f1\n",
    "    }\n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 95.99282296650718,\n",
       " 'precision': 0.9616945511206245,\n",
       " 'recall': 0.9599282296650717,\n",
       " 'f1': 0.9567614211061143}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_results = calculate_results(y_true = x_test , y_pred=baseline_preds)\n",
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow model for the model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape=(1,),dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "outputs = layers.Dense(1,activation=\"sigmoid\")(x)\n",
    "model_1 = tf.keras.Model(inputs , outputs ,name = \"model_1_densse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_densse\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_2 (Text  (None, 15)                0         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 128)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1280129 (4.88 MB)\n",
      "Trainable params: 1280129 (4.88 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "122/122 [==============================] - 10s 51ms/step - loss: 0.0401 - accuracy: 0.9892 - val_loss: 0.0845 - val_accuracy: 0.9761\n",
      "Epoch 2/5\n",
      "122/122 [==============================] - 5s 43ms/step - loss: 0.0281 - accuracy: 0.9915 - val_loss: 0.0821 - val_accuracy: 0.9773\n",
      "Epoch 3/5\n",
      "122/122 [==============================] - 5s 40ms/step - loss: 0.0207 - accuracy: 0.9941 - val_loss: 0.0814 - val_accuracy: 0.9785\n",
      "Epoch 4/5\n",
      "122/122 [==============================] - 5s 42ms/step - loss: 0.0157 - accuracy: 0.9959 - val_loss: 0.0823 - val_accuracy: 0.9785\n",
      "Epoch 5/5\n",
      "122/122 [==============================] - 5s 42ms/step - loss: 0.0118 - accuracy: 0.9967 - val_loss: 0.0838 - val_accuracy: 0.9785\n"
     ]
    }
   ],
   "source": [
    "model_1_history = model_1.fit(\n",
    "    y_train,\n",
    "    x_train,\n",
    "    epochs = 5,\n",
    "    validation_data = (y_test , x_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 4ms/step - loss: 0.0927 - accuracy: 0.9725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09269191324710846, 0.9724880456924438]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.evaluate(y_test , x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 128)\n"
     ]
    }
   ],
   "source": [
    "embed_weights = model_1.get_layer(\"embedding\").get_weights()[0]\n",
    "print(embed_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 3s 15ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6.7256123e-01],\n",
       "       [1.3051700e-02],\n",
       "       [9.7295398e-01],\n",
       "       [5.2309531e-01],\n",
       "       [9.9999255e-01],\n",
       "       [3.0881870e-03],\n",
       "       [2.0475173e-02],\n",
       "       [1.3739296e-03],\n",
       "       [3.5868652e-05],\n",
       "       [8.3282609e-03]], dtype=float32)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_pred_probs = model_1.predict(y_test)\n",
    "model_1_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
       "array([1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_preds = tf.squeeze(tf.round(model_1_pred_probs))\n",
    "model_1_preds[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 97.84688995215312,\n",
       " 'precision': 0.9783919782794124,\n",
       " 'recall': 0.9784688995215312,\n",
       " 'f1': 0.9778849854534022}"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_results = calculate_results(y_true=x_test,\n",
    "                                    y_pred=model_1_preds)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7829, ['', '[UNK]', 'to', 'i', 'you', 'a', 'the', 'u', 'and', 'in'])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "len(words_in_vocab), words_in_vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 128)\n"
     ]
    }
   ],
   "source": [
    "embed_weights = model_1.get_layer(\"embedding\").get_weights()[0]\n",
    "print(embed_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize the data in the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code below is adapted from: https://www.tensorflow.org/tutorials/text/word_embeddings#retrieve_the_trained_word_embeddings_and_save_them_to_disk\n",
    "import io\n",
    "\n",
    "# Create output writers\n",
    "out_v = io.open(\"embedding_vectors.tsv\", \"w\", encoding=\"utf-8\")\n",
    "out_m = io.open(\"embedding_metadata.tsv\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "# Write embedding vectors and words to file\n",
    "for num, word in enumerate(words_in_vocab):\n",
    "  if num == 0:\n",
    "     continue # skip padding token\n",
    "  vec = embed_weights[num]\n",
    "  out_m.write(word + \"\\n\") # write words to file\n",
    "  out_v.write(\"\\t\".join([str(x) for x in vec]) + \"\\n\") # write corresponding word vector to file\n",
    "out_v.close()\n",
    "out_m.close()\n",
    "\n",
    "# # Download files locally to upload to Embedding Projector\n",
    "# try:\n",
    "#   from google.colab import files\n",
    "# except ImportError:\n",
    "#   pass\n",
    "# else:\n",
    "#   files.download(\"embedding_vectors.tsv\")\n",
    "#   files.download(\"embedding_metadata.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 15, 128)\n",
      "(None, 128)\n"
     ]
    }
   ],
   "source": [
    "# Set random seed and create embedding layer (new embedding layer for each model)\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers\n",
    "model_2_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                     output_dim=128,\n",
    "                                     embeddings_initializer=\"uniform\",\n",
    "                                     input_length=max_length,\n",
    "                                     name=\"embedding_2\")\n",
    "\n",
    "# Create LSTM model\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = model_2_embedding(x)\n",
    "print(x.shape)\n",
    "# x = layers.LSTM(64, return_sequences=True)(x) # return vector for each word in the Tweet (you can stack RNN cells as long as return_sequences=True)\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x) # return vector for whole sequence\n",
    "print(x.shape)\n",
    "# x = layers.GlobalAveragePooling1D()(x)\n",
    "# x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer on top of output of LSTM cell\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_2 = tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model_2.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "122/122 [==============================] - 25s 108ms/step - loss: 0.0015 - accuracy: 0.9992 - val_loss: 0.1370 - val_accuracy: 0.9761\n",
      "Epoch 2/4\n",
      "122/122 [==============================] - 9s 77ms/step - loss: 6.4542e-04 - accuracy: 0.9997 - val_loss: 0.1456 - val_accuracy: 0.9767\n",
      "Epoch 3/4\n",
      "122/122 [==============================] - 10s 78ms/step - loss: 8.6022e-05 - accuracy: 1.0000 - val_loss: 0.1391 - val_accuracy: 0.9755\n",
      "Epoch 4/4\n",
      "122/122 [==============================] - 9s 77ms/step - loss: 1.3060e-05 - accuracy: 1.0000 - val_loss: 0.1470 - val_accuracy: 0.9755\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "model_2_history = model_2.fit(y_train,\n",
    "                              x_train,\n",
    "                              epochs=4,\n",
    "                              validation_data=(y_test, x_test),\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 7s 17ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1672, 1),\n",
       " array([[6.7699458e-03],\n",
       "        [6.5633375e-04],\n",
       "        [9.9921429e-01],\n",
       "        [7.1226083e-02],\n",
       "        [9.9999845e-01],\n",
       "        [1.8746797e-06],\n",
       "        [6.2745953e-06],\n",
       "        [2.1606957e-06],\n",
       "        [2.4330791e-06],\n",
       "        [6.1751971e-06]], dtype=float32))"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on the validation dataset\n",
    "model_2_pred_probs = model_2.predict(y_test)\n",
    "model_2_pred_probs.shape, model_2_pred_probs[:10] # view the first 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 97.54784688995215,\n",
       " 'precision': 0.9751450493645157,\n",
       " 'recall': 0.9754784688995215,\n",
       " 'f1': 0.9748952343685159}"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2_result = calculate_2_results = calculate_results(y_true = x_test,\n",
    "                                        y_pred = model_2_preds)\n",
    "model_2_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1. 0. 1. ... 0. 0. 0.], shape=(1672,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# type(y_test[:0])\n",
    "model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\n",
    "print(model_2_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 125ms/step\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Your input text\n",
    "input_text = \"how are you\"\n",
    "\n",
    "# Create a pandas Series from the input text\n",
    "series = pd.Series([input_text])\n",
    "\n",
    "model_sample_pred_probs = model_2.predict(series)\n",
    "model_sample_preds = tf.squeeze(tf.round(model_sample_pred_probs))\n",
    "# The variable model_2_pred_probs now contains the predicted probabilities\n",
    "print(model_sample_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflowjs\n",
      "  Obtaining dependency information for tensorflowjs from https://files.pythonhosted.org/packages/78/77/f9a83027eca63ac777daf3c133a53f77c47139a25d236629d5634b0e2025/tensorflowjs-4.10.0-py3-none-any.whl.metadata\n",
      "  Downloading tensorflowjs-4.10.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting flax<0.6.3,>=0.6.2 (from tensorflowjs)\n",
      "  Using cached flax-0.6.2-py3-none-any.whl (189 kB)\n",
      "Collecting importlib_resources>=5.9.0 (from tensorflowjs)\n",
      "  Obtaining dependency information for importlib_resources>=5.9.0 from https://files.pythonhosted.org/packages/25/d4/592f53ce2f8dde8be5720851bd0ab71cc2e76c55978e4163ef1ab7e389bb/importlib_resources-6.0.1-py3-none-any.whl.metadata\n",
      "  Downloading importlib_resources-6.0.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting jax>=0.3.16 (from tensorflowjs)\n",
      "  Downloading jax-0.4.14.tar.gz (1.3 MB)\n",
      "     ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.0/1.3 MB 326.8 kB/s eta 0:00:04\n",
      "     - -------------------------------------- 0.1/1.3 MB 363.1 kB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.1/1.3 MB 357.2 kB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.1/1.3 MB 327.7 kB/s eta 0:00:04\n",
      "     --- ------------------------------------ 0.1/1.3 MB 344.8 kB/s eta 0:00:04\n",
      "     --- ------------------------------------ 0.1/1.3 MB 343.4 kB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 0.1/1.3 MB 370.8 kB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 0.1/1.3 MB 370.8 kB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 0.2/1.3 MB 305.7 kB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 0.2/1.3 MB 327.7 kB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 0.2/1.3 MB 327.7 kB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 0.2/1.3 MB 327.7 kB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 0.2/1.3 MB 327.7 kB/s eta 0:00:04\n",
      "     ------ --------------------------------- 0.2/1.3 MB 305.9 kB/s eta 0:00:04\n",
      "     ------- -------------------------------- 0.2/1.3 MB 300.4 kB/s eta 0:00:04\n",
      "     ------- -------------------------------- 0.2/1.3 MB 300.4 kB/s eta 0:00:04\n",
      "     ------- -------------------------------- 0.2/1.3 MB 300.4 kB/s eta 0:00:04\n",
      "     ------- -------------------------------- 0.3/1.3 MB 275.8 kB/s eta 0:00:04\n",
      "     -------- ------------------------------- 0.3/1.3 MB 285.3 kB/s eta 0:00:04\n",
      "     --------- ------------------------------ 0.3/1.3 MB 301.8 kB/s eta 0:00:04\n",
      "     ---------- ----------------------------- 0.3/1.3 MB 313.0 kB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 0.4/1.3 MB 323.2 kB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 0.4/1.3 MB 323.2 kB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 0.4/1.3 MB 323.3 kB/s eta 0:00:03\n",
      "     ------------ --------------------------- 0.4/1.3 MB 323.5 kB/s eta 0:00:03\n",
      "     ------------ --------------------------- 0.4/1.3 MB 323.5 kB/s eta 0:00:03\n",
      "     ------------ --------------------------- 0.4/1.3 MB 323.5 kB/s eta 0:00:03\n",
      "     ------------ --------------------------- 0.4/1.3 MB 323.5 kB/s eta 0:00:03\n",
      "     ------------ --------------------------- 0.4/1.3 MB 323.5 kB/s eta 0:00:03\n",
      "     -------------- ------------------------- 0.5/1.3 MB 324.2 kB/s eta 0:00:03\n",
      "     -------------- ------------------------- 0.5/1.3 MB 324.2 kB/s eta 0:00:03\n",
      "     -------------- ------------------------- 0.5/1.3 MB 314.4 kB/s eta 0:00:03\n",
      "     --------------- ------------------------ 0.5/1.3 MB 318.0 kB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 0.5/1.3 MB 321.5 kB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 0.5/1.3 MB 321.5 kB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 0.6/1.3 MB 327.7 kB/s eta 0:00:03\n",
      "     ------------------ --------------------- 0.6/1.3 MB 330.5 kB/s eta 0:00:03\n",
      "     ------------------ --------------------- 0.6/1.3 MB 327.7 kB/s eta 0:00:03\n",
      "     ------------------- -------------------- 0.6/1.3 MB 330.3 kB/s eta 0:00:03\n",
      "     ------------------- -------------------- 0.6/1.3 MB 330.3 kB/s eta 0:00:03\n",
      "     ------------------- -------------------- 0.6/1.3 MB 330.3 kB/s eta 0:00:03\n",
      "     ------------------- -------------------- 0.6/1.3 MB 330.3 kB/s eta 0:00:03\n",
      "     -------------------- ------------------- 0.7/1.3 MB 310.9 kB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 0.8/1.3 MB 353.6 kB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 0.8/1.3 MB 348.4 kB/s eta 0:00:02\n",
      "     ------------------------ --------------- 0.8/1.3 MB 352.5 kB/s eta 0:00:02\n",
      "     ------------------------ --------------- 0.8/1.3 MB 345.1 kB/s eta 0:00:02\n",
      "     ------------------------- -------------- 0.8/1.3 MB 346.9 kB/s eta 0:00:02\n",
      "     ------------------------- -------------- 0.9/1.3 MB 350.9 kB/s eta 0:00:02\n",
      "     -------------------------- ------------- 0.9/1.3 MB 356.7 kB/s eta 0:00:02\n",
      "     -------------------------- ------------- 0.9/1.3 MB 356.7 kB/s eta 0:00:02\n",
      "     --------------------------- ------------ 0.9/1.3 MB 353.5 kB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 0.9/1.3 MB 357.2 kB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 1.0/1.3 MB 366.3 kB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 1.0/1.3 MB 366.3 kB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 1.0/1.3 MB 366.3 kB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 1.0/1.3 MB 366.3 kB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 1.0/1.3 MB 366.3 kB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 1.0/1.3 MB 366.3 kB/s eta 0:00:01\n",
      "     --------------------------------- ------ 1.1/1.3 MB 380.0 kB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 1.1/1.3 MB 376.9 kB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 1.2/1.3 MB 383.1 kB/s eta 0:00:01\n",
      "     ------------------------------------ --- 1.2/1.3 MB 385.4 kB/s eta 0:00:01\n",
      "     ------------------------------------- -- 1.2/1.3 MB 389.4 kB/s eta 0:00:01\n",
      "     ---------------------------------------  1.3/1.3 MB 404.0 kB/s eta 0:00:01\n",
      "     ---------------------------------------  1.3/1.3 MB 403.3 kB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.3/1.3 MB 399.1 kB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tensorflow<3,>=2.12.0 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from tensorflowjs) (2.13.0)\n",
      "INFO: pip is looking at multiple versions of tensorflowjs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorflowjs\n",
      "  Obtaining dependency information for tensorflowjs from https://files.pythonhosted.org/packages/29/f8/f83b725878d8bd3305c7674221ee158b272cfed0ef3a7110e59f31cbcc13/tensorflowjs-4.9.0-py3-none-any.whl.metadata\n",
      "  Downloading tensorflowjs-4.9.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "  Obtaining dependency information for tensorflowjs from https://files.pythonhosted.org/packages/a2/8b/3fc4ec2e725fb6d50073590ecf987784add309488d11ad2a3203f77f4a68/tensorflowjs-4.8.0-py3-none-any.whl.metadata\n",
      "  Downloading tensorflowjs-4.8.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "  Obtaining dependency information for tensorflowjs from https://files.pythonhosted.org/packages/aa/7a/74e998917c69be308979f22291e10c5903f865f7902b5ff4d545e4d335c1/tensorflowjs-4.7.0-py3-none-any.whl.metadata\n",
      "  Downloading tensorflowjs-4.7.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "  Obtaining dependency information for tensorflowjs from https://files.pythonhosted.org/packages/10/7b/d61f39cbae66602d9fb43e73f0bd0183e9736cdb09391daeb954f90e4a53/tensorflowjs-4.6.0-py3-none-any.whl.metadata\n",
      "  Downloading tensorflowjs-4.6.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "  Using cached tensorflowjs-4.5.0-py3-none-any.whl (85 kB)\n",
      "  Using cached tensorflowjs-4.4.0-py3-none-any.whl (85 kB)\n",
      "Collecting flax>=0.6.2 (from tensorflowjs)\n",
      "  Obtaining dependency information for flax>=0.6.2 from https://files.pythonhosted.org/packages/72/a7/147bd0682ff39a4e59352506a7d858e1e003f05a2d96e431fabb8a5491e4/flax-0.7.2-py3-none-any.whl.metadata\n",
      "  Downloading flax-0.7.2-py3-none-any.whl.metadata (10.0 kB)\n",
      "Collecting protobuf<3.20,>=3.9.2 (from tensorflowjs)\n",
      "  Using cached protobuf-3.19.6-py2.py3-none-any.whl (162 kB)\n",
      "Collecting tensorflowjs\n",
      "  Using cached tensorflowjs-4.3.0-py3-none-any.whl (85 kB)\n",
      "INFO: pip is still looking at multiple versions of tensorflowjs to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached tensorflowjs-4.2.0-py3-none-any.whl (84 kB)\n",
      "  Using cached tensorflowjs-4.1.0-py3-none-any.whl (84 kB)\n",
      "  Using cached tensorflowjs-4.0.0-py3-none-any.whl (83 kB)\n",
      "  Using cached tensorflowjs-3.21.0-py3-none-any.whl (81 kB)\n",
      "Requirement already satisfied: six<2,>=1.12.0 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from tensorflowjs) (1.16.0)\n",
      "Collecting tensorflow-hub<0.13,>=0.7.0 (from tensorflowjs)\n",
      "  Using cached tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
      "Collecting packaging~=20.9 (from tensorflowjs)\n",
      "  Using cached packaging-20.9-py2.py3-none-any.whl (40 kB)\n",
      "Requirement already satisfied: numpy>=1.12 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from flax>=0.6.2->tensorflowjs) (1.25.2)\n",
      "Collecting msgpack (from flax>=0.6.2->tensorflowjs)\n",
      "  Using cached msgpack-1.0.5-cp311-cp311-win_amd64.whl (60 kB)\n",
      "Collecting optax (from flax>=0.6.2->tensorflowjs)\n",
      "  Obtaining dependency information for optax from https://files.pythonhosted.org/packages/13/71/787cc24c4b606f3bb9f1d14957ebd7cb9e4234f6d59081721230b2032196/optax-0.1.7-py3-none-any.whl.metadata\n",
      "  Downloading optax-0.1.7-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting orbax-checkpoint (from flax>=0.6.2->tensorflowjs)\n",
      "  Obtaining dependency information for orbax-checkpoint from https://files.pythonhosted.org/packages/8c/e9/6ad0ceb896df52d46c58587f99df6251aaec3ee458764252cc6305e8e06b/orbax_checkpoint-0.3.5-py3-none-any.whl.metadata\n",
      "  Downloading orbax_checkpoint-0.3.5-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorstore (from flax>=0.6.2->tensorflowjs)\n",
      "  Obtaining dependency information for tensorstore from https://files.pythonhosted.org/packages/3d/c7/65e27f519196e97bad76a759bf7cc44696fd2ecb3caefd58466604ac60dd/tensorstore-0.1.41-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading tensorstore-0.1.41-cp311-cp311-win_amd64.whl.metadata (3.0 kB)\n",
      "Collecting rich>=11.1 (from flax>=0.6.2->tensorflowjs)\n",
      "  Obtaining dependency information for rich>=11.1 from https://files.pythonhosted.org/packages/8d/5f/21a93b2ec205f4b79853ff6e838e3c99064d5dbe85ec6b05967506f14af0/rich-13.5.2-py3-none-any.whl.metadata\n",
      "  Downloading rich-13.5.2-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from flax>=0.6.2->tensorflowjs) (4.5.0)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from flax>=0.6.2->tensorflowjs) (6.0.1)\n",
      "Collecting ml-dtypes>=0.2.0 (from jax>=0.3.16->tensorflowjs)\n",
      "  Obtaining dependency information for ml-dtypes>=0.2.0 from https://files.pythonhosted.org/packages/08/89/c727fde1a3d12586e0b8c01abf53754707d76beaa9987640e70807d4545f/ml_dtypes-0.2.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading ml_dtypes-0.2.0-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from jax>=0.3.16->tensorflowjs) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.7 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from jax>=0.3.16->tensorflowjs) (1.11.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from packaging~=20.9->tensorflowjs) (3.0.9)\n",
      "Requirement already satisfied: tensorflow-intel==2.13.0 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (2.13.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (16.0.6)\n",
      "Collecting numpy>=1.12 (from flax>=0.6.2->tensorflowjs)\n",
      "  Using cached numpy-1.24.3-cp311-cp311-win_amd64.whl (14.8 MB)\n",
      "INFO: pip is looking at multiple versions of tensorflow-intel to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorflow<3,>=2.1.0 (from tensorflowjs)\n",
      "  Obtaining dependency information for tensorflow<3,>=2.1.0 from https://files.pythonhosted.org/packages/9e/b8/ed5f794359d05cd0bffb894c6418da87b93016ee17b669d55c45d1bd5d5b/tensorflow-2.13.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached tensorflow-2.13.0-cp311-cp311-win_amd64.whl.metadata (2.6 kB)\n",
      "  Obtaining dependency information for tensorflow<3,>=2.1.0 from https://files.pythonhosted.org/packages/ca/a9/8cf654c2a89ea0103f3d3ea6f953b425d22f98736f7c2d2ebccf863b6d31/tensorflow-2.12.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading tensorflow-2.12.1-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting tensorflow-intel==2.12.1 (from tensorflow<3,>=2.1.0->tensorflowjs)\n",
      "  Obtaining dependency information for tensorflow-intel==2.12.1 from https://files.pythonhosted.org/packages/d4/60/1ca023f19c8c22e283f5fd3fd209dbe11c8fb7a5078fc4ff247738eab74d/tensorflow_intel-2.12.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading tensorflow_intel-2.12.1-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting tensorflow<3,>=2.1.0 (from tensorflowjs)\n",
      "  Using cached tensorflow-2.12.0-cp311-cp311-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.12.0 (from tensorflow<3,>=2.1.0->tensorflowjs)\n",
      "  Using cached tensorflow_intel-2.12.0-cp311-cp311-win_amd64.whl (272.9 MB)\n",
      "Collecting numpy>=1.12 (from flax>=0.6.2->tensorflowjs)\n",
      "  Using cached numpy-1.23.5-cp311-cp311-win_amd64.whl (14.6 MB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting tensorflowjs\n",
      "  Using cached tensorflowjs-3.20.0-py3-none-any.whl (81 kB)\n",
      "INFO: pip is still looking at multiple versions of tensorflow-intel to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached tensorflowjs-3.19.0-py3-none-any.whl (78 kB)\n",
      "Collecting protobuf==3.20.0 (from tensorflowjs)\n",
      "  Using cached protobuf-3.20.0-py2.py3-none-any.whl (162 kB)\n",
      "Collecting tensorflowjs\n",
      "  Using cached tensorflowjs-3.18.0-py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (4.24.0)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\python311\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (65.5.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (2.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (1.15.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (1.57.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (2.13.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (2.13.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (0.41.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (3.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (2.3.7)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (1.26.16)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\win10\\appdata\\roaming\\python\\python311\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow<3,>=2.12.0->tensorflowjs) (3.2.2)\n",
      "Installing collected packages: packaging, numpy, tensorflow-hub, tensorflowjs\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.1\n",
      "    Uninstalling packaging-23.1:\n",
      "      Successfully uninstalled packaging-23.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.25.2\n",
      "    Uninstalling numpy-1.25.2:\n",
      "      Successfully uninstalled numpy-1.25.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\win10\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\~-mpy\\\\.libs\\\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll'\n",
      "Check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflowjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win10\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflowjs\\read_weights.py:28: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  np.uint8, np.uint16, np.object, np.bool]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\win10\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\win10\\AppData\\Local\\Temp\\ipykernel_8728\\1219052119.py\", line 1, in <module>\n",
      "    import tensorflowjs as tfjs\n",
      "  File \"C:\\Users\\win10\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflowjs\\__init__.py\", line 21, in <module>\n",
      "    from tensorflowjs import converters\n",
      "  File \"C:\\Users\\win10\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflowjs\\converters\\__init__.py\", line 21, in <module>\n",
      "    from tensorflowjs.converters.converter import convert\n",
      "  File \"C:\\Users\\win10\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflowjs\\converters\\converter.py\", line 35, in <module>\n",
      "    from tensorflowjs.converters import keras_h5_conversion as conversion\n",
      "  File \"C:\\Users\\win10\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflowjs\\converters\\keras_h5_conversion.py\", line 33, in <module>\n",
      "    from tensorflowjs import write_weights  # pylint: disable=import-error\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\win10\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflowjs\\write_weights.py\", line 25, in <module>\n",
      "    from tensorflowjs import read_weights\n",
      "  File \"C:\\Users\\win10\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflowjs\\read_weights.py\", line 28, in <module>\n",
      "    np.uint8, np.uint16, np.object, np.bool]\n",
      "                         ^^^^^^^^^\n",
      "  File \"C:\\Users\\win10\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\__init__.py\", line 319, in __getattr__\n",
      "AttributeError: module 'numpy' has no attribute 'object'.\n",
      "`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\win10\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\win10\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1428, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\win10\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1319, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\win10\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1172, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\win10\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1087, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\win10\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 969, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\win10\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\win10\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\win10\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\win10\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\win10\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\win10\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\win10\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"C:\\Users\\win10\\AppData\\Roaming\\Python\\Python311\\site-packages\\executing\\executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import tensorflowjs as tfjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfjs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[211], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m tfjs\u001b[39m.\u001b[39mconverters\u001b[39m.\u001b[39msave_keras_model(\n\u001b[0;32m      3\u001b[0m         model_2,\n\u001b[0;32m      4\u001b[0m         os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m/models\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mspamModel/\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m         )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfjs' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "tfjs.converters.save_keras_model(\n",
    "        model_2,\n",
    "        os.path.join('/models','spamModel/')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_test[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5_Conv1D\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_2 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding_5 (Embedding)     (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 11, 32)            20512     \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 32)                0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1300545 (4.96 MB)\n",
      "Trainable params: 1300545 (4.96 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Set random seed and create embedding layer (new embedding layer for each model)\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers\n",
    "model_5_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
    "                                     output_dim=128,\n",
    "                                     embeddings_initializer=\"uniform\",\n",
    "                                     input_length=max_length,\n",
    "                                     name=\"embedding_5\")\n",
    "\n",
    "# Create 1-dimensional convolutional layer to model sequences\n",
    "from tensorflow.keras import layers\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = model_5_embedding(x)\n",
    "x = layers.Conv1D(filters=32, kernel_size=5, activation=\"relu\")(x)\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "# x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_5 = tf.keras.Model(inputs, outputs, name=\"model_5_Conv1D\")\n",
    "\n",
    "# Compile Conv1D model\n",
    "model_5.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# Get a summary of our 1D convolution model\n",
    "model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "122/122 [==============================] - 8s 49ms/step - loss: 0.2944 - accuracy: 0.8897 - val_loss: 0.1386 - val_accuracy: 0.9665\n",
      "Epoch 2/5\n",
      "122/122 [==============================] - 6s 48ms/step - loss: 0.0754 - accuracy: 0.9808 - val_loss: 0.0877 - val_accuracy: 0.9755\n",
      "Epoch 3/5\n",
      "122/122 [==============================] - 6s 45ms/step - loss: 0.0263 - accuracy: 0.9936 - val_loss: 0.0840 - val_accuracy: 0.9761\n",
      "Epoch 4/5\n",
      "122/122 [==============================] - 6s 47ms/step - loss: 0.0107 - accuracy: 0.9982 - val_loss: 0.0859 - val_accuracy: 0.9755\n",
      "Epoch 5/5\n",
      "122/122 [==============================] - 6s 49ms/step - loss: 0.0055 - accuracy: 0.9992 - val_loss: 0.0896 - val_accuracy: 0.9773\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_5_history = model_5.fit(y_train,\n",
    "                              x_train,\n",
    "                              epochs=5,\n",
    "                              validation_data=(y_test, x_test)\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 1s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1672, 1),\n",
       " array([[9.4827724e-01],\n",
       "        [4.6196266e-04],\n",
       "        [9.6759254e-01],\n",
       "        [6.4812154e-01],\n",
       "        [9.9999654e-01],\n",
       "        [1.6038724e-03],\n",
       "        [2.8326563e-03],\n",
       "        [2.5044777e-03],\n",
       "        [2.9368882e-04],\n",
       "        [2.8296788e-03]], dtype=float32))"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on the validation dataset\n",
    "model_5_pred_probs = model_5.predict(y_test)\n",
    "model_5_pred_probs.shape, model_5_pred_probs[:10] # view the first 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1672,), dtype=float32, numpy=array([1., 0., 1., ..., 0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5_preds = tf.squeeze(tf.round(model_5_pred_probs))\n",
    "model_5_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 97.72727272727273,\n",
       " 'precision': 0.97700601472003,\n",
       " 'recall': 0.9772727272727273,\n",
       " 'f1': 0.9767571482479271}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5_results = calculate_results(y_true=x_test,\n",
    "                                    y_pred = model_5_preds)\n",
    "model_5_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 128)\n"
     ]
    }
   ],
   "source": [
    "embed_weights2 = model_5.get_layer(\"embedding_5\").get_weights()[0]\n",
    "print(embed_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code below is adapted from: https://www.tensorflow.org/tutorials/text/word_embeddings#retrieve_the_trained_word_embeddings_and_save_them_to_disk\n",
    "import io\n",
    "\n",
    "# Create output writers\n",
    "out_v = io.open(\"embedding_vectors5.tsv\", \"w\", encoding=\"utf-8\")\n",
    "out_m = io.open(\"embedding_metadata5.tsv\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "# Write embedding vectors and words to file\n",
    "for num, word in enumerate(words_in_vocab):\n",
    "  if num == 0:\n",
    "     continue # skip padding token\n",
    "  vec = embed_weights2[num]\n",
    "  out_m.write(word + \"\\n\") # write words to file\n",
    "  out_v.write(\"\\t\".join([str(x) for x in vec]) + \"\\n\") # write corresponding word vector to file\n",
    "out_v.close()\n",
    "out_m.close()\n",
    "\n",
    "# # Download files locally to upload to Embedding Projector\n",
    "# try:\n",
    "#   from google.colab import files\n",
    "# except ImportError:\n",
    "#   pass\n",
    "# else:\n",
    "#   files.download(\"embedding_vectors.tsv\")\n",
    "#   files.download(\"embedding_metadata.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
